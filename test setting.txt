# agent1 settings: with dense reward - node4
######################################
max_frames  = 8e6
max_steps   = 50
batch_size  = 128
num_updates=10
num_agents=2
checkpoints_interval=10000
evaluation_attempts=5
warm_up=20000
stored_points_for_cell=10
stored_trajectories_length=40
alpha=0.5
alpha_decay=0.9999994

replay_buffer_size = 1e5
hidden_size=128
actor_learning_rate=3e-4
critic_learning_rate=1e-3
epsilon_decay=0.9999995
epsilon=1
noise_scale=0.1
RRT_budget=35
max_steps   = 50
minimum_exploration=0.01
rrt_prob=0.6
short_memory_size=int(1e3)
short_memory_updates=5
tau=1e-2
gamma=0.98
rrt_min_visit=100
######################################


# agent2 settings: with dense reward - node3
######################################
max_frames  = 8e6
max_steps   = 50
batch_size  = 128
num_updates=10
num_agents=2
checkpoints_interval=10000
evaluation_attempts=5
warm_up=20000
stored_points_for_cell=10
stored_trajectories_length=40
alpha=0.5
alpha_decay=0.9999994

replay_buffer_size = 1e5
hidden_size=128
actor_learning_rate=1e-4
critic_learning_rate=1e-3
epsilon_decay=0.9999995
epsilon=1
noise_scale=0.1
RRT_budget=35
max_steps   = 50
minimum_exploration=0.01
rrt_prob=0.6
short_memory_size=int(1e3)
short_memory_updates=5
tau=1e-2
gamma=0.98
rrt_min_visit=100
######################################